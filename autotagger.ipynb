{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/utils/__init__.py:4: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/heniozicukier/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/heniozicukier/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/heniozicukier/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "import csv\n",
    "import nltk\n",
    "import json\n",
    "# import logging, gensim\n",
    "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "nltk.download('punkt')   # tokenizer\n",
    "nltk.download('averaged_perceptron_tagger') # POS\n",
    "nltk.download('wordnet') # similarity\n",
    "\n",
    "def with_tag(bookmarks, tag):\n",
    "    return list(filter(lambda item: tag in item.tags, bookmarks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bookmark(title='deepnight.net', tags=['compsci', 'gaming'])\n",
      "Bookmark(title='How to Prototype a Game in Under 7 Days', tags=['gamedev'])\n",
      "Bookmark(title='Java and XML - Tutorial', tags=['compsci', 'java'])\n",
      "Bookmark(title='Plain English explanation of Big O', tags=['compsci', 'maths'])\n",
      "Bookmark(title='Mathigon | World of Mathematics', tags=['compsci', 'maths'])\n"
     ]
    }
   ],
   "source": [
    "bookmarks = []\n",
    "Bookmark = namedtuple('Bookmark', ['title', 'tags'])\n",
    "\n",
    "with open('marks.csv', newline='', encoding=\"utf8\") as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "    # transforms lists as strings to real lists\n",
    "    for row in reader:\n",
    "        bookmarks.append(Bookmark(row[0], eval(row[1])))\n",
    "        \n",
    "for b in bookmarks[100:105]: print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bookmark(title='“Right click and save as” needs to go away', tags=['compsci'])\n",
      "Bookmark(title='Fun with Java2D - Strokes', tags=['compsci'])\n",
      "Bookmark(title='Books for Computer Science Graduate Students', tags=['compsci'])\n",
      "Bookmark(title='Chrome Extensions Intro', tags=['compsci'])\n",
      "Bookmark(title=\"Google's Python Class\", tags=['compsci', 'python'])\n"
     ]
    }
   ],
   "source": [
    "for b in with_tag(bookmarks, \"compsci\")[:5]:\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['people', 'ponte', 'travel', 'haskell', 'vim', 'c', 'philosophy', 'synth', '.jobs', 'algorithms', 'clojure', '.unlisted', 'interactive', 'mix', 'sql', 'wikipedia', 'proglangs', 'site', 'articles', 'tvtropes', 'music', 'ai', 'foundry', 'max', 'tcc', 'swift', 'functional', 'academia', 'webdev', 'hardware', 'js', 'history', 'private', 'typography', 'interviews', 'now', 'misc', 'maths', 'assembly', 'gamedev', 'gaming', '.apto', 'production', 'nlp', 'tweet', 'lists', 'music-theory', 'golang', 'grybo', 'film', 'animation', 'acoustics', 'nix', 'gallery', 'art', 'cah', 'ice', 'papers', '3d', 'graphics', 'java', 'inpe', 'food', 'networks', 'dsp', 'rym', 'photography', 'jazz', 'cpp', 'videos', '.tab-collection', 'programming', 'audiodev', 'infosec', 'birmingham', 'process', '.lucio', 'python', 'haha', 'later', 'awesome', 'instruments', 'css', 'tex', 'guitar', 'compsci', '.shopping', '.sp', '.campanha', 'installation', 'architecture', 'electronics', 'illustration', 'books', 'data', 'crypto', 'lisp', 'ux', 'blogs', 'design', 'viz', 'colors', 'bmarks', 'language']\n"
     ]
    }
   ],
   "source": [
    "all_tags = []\n",
    "for bookmark in bookmarks:\n",
    "    all_tags.extend(bookmark.tags)\n",
    "    \n",
    "all_tags = list(set(all_tags))\n",
    "print(all_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 2380\n",
      "Test set:  595\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "bookmarks_train, bookmarks_test = train_test_split(bookmarks, \n",
    "                                                   test_size=0.20, \n",
    "                                                   random_state=42)\n",
    "\n",
    "print(\"Train set:\", len(bookmarks_train))\n",
    "print(\"Test set: \", len(bookmarks_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tamask', 'NN'), (\"'s\", 'POS'), ('(', '('), ('KR0', 'NNP'), (')', ')'), ('Gists', 'VBZ')]\n",
      "[('Framer', 'NNP'), ('-', ':'), ('Animation', 'NN'), ('Prototyping', 'VBG'), ('Tool', 'NN')]\n",
      "[('Open', 'NNP'), ('Source', 'NNP'), ('Game', 'NNP'), ('Clones', 'NNP')]\n",
      "[('A', 'DT'), ('few', 'JJ'), ('useful', 'JJ'), ('things', 'NNS'), ('to', 'TO'), ('know', 'VB'), ('about', 'IN'), ('machine', 'NN'), ('learning', 'NN')]\n",
      "[('You', 'PRP'), ('Might', 'MD'), ('Not', 'RB'), ('Need', 'VB'), ('jQuery', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "for bookmark in with_tag(bookmarks_train, \"compsci\")[:5]:\n",
    "    tokenized = nltk.word_tokenize(bookmark.title)\n",
    "    print(nltk.pos_tag(tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "pst = PorterStemmer()\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "relevant_pos = [\n",
    "    \"NNP\",\n",
    "    \"NN\",\n",
    "    \"NNS\",\n",
    "    \"JJ\",\n",
    "]\n",
    "\n",
    "def train(stem_fn):\n",
    "    tags_stems = defaultdict(lambda: defaultdict(int))\n",
    "    stems_tags = defaultdict(lambda: defaultdict(int))\n",
    "    words = []\n",
    "\n",
    "    for bookmark in bookmarks_train:\n",
    "        tokenized = nltk.word_tokenize(bookmark.title)\n",
    "        for word, pos in nltk.pos_tag(tokenized):\n",
    "            if pos in relevant_pos:\n",
    "                stem = stem_fn(word.lower())\n",
    "                if re.match(r\"^[\\w\\d.-]+$\", stem):\n",
    "                    for tag in bookmark.tags:\n",
    "                        tags_stems[tag][stem] += 1\n",
    "                        stems_tags[stem][tag] += 1\n",
    "                    words.append(stem)\n",
    "                    \n",
    "    return (tags_stems, stems_tags)\n",
    "    \n",
    "# tags_stems, stems_tags = train(porter_stemmer.stem)\n",
    "tags_stems, stems_tags = train(pst.stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('comput', 33), ('game', 23), ('python', 19), ('algorithm', 17), ('learn', 16), ('guid', 16), ('program', 13), ('book', 12), ('languag', 12), ('music', 12)]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(tags_stems[\"compsci\"].items(),\n",
    "             key=lambda k_v: k_v[1],\n",
    "             reverse=True)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('programming', 2), ('compsci', 2), ('lists', 2), ('music', 2), ('production', 2), ('audiodev', 1), ('design', 1), ('swift', 1)]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(stems_tags[\"plugin\"].items(),\n",
    "             key=lambda k_v: k_v[1],\n",
    "             reverse=True)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def autotag(title, tags_stems, stem_fn):\n",
    "    stems = []\n",
    "    tokenized = nltk.word_tokenize(title)\n",
    "    for word, pos in nltk.pos_tag(tokenized):\n",
    "        if pos in relevant_pos:\n",
    "            stem = stem_fn(word)\n",
    "            if re.match(r\"^[\\w\\d.-]+$\", stem):\n",
    "                stems.append(stem)\n",
    "    \n",
    "    tags_points = defaultdict(int)\n",
    "    \n",
    "    for stem in stems:\n",
    "        for tag in tags_stems:\n",
    "            for t_stem, count in tags_stems[tag].items():\n",
    "                if stem == t_stem:\n",
    "                    tags_points[tag] += count\n",
    "                    \n",
    "    return list(itertools.islice(sorted(tags_points.items(), \n",
    "                                        key=lambda kv: -kv[1]), \n",
    "                                 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 discos importantes da música brasileira\n",
      "Computed:  [('music', 11), ('lists', 4), ('articles', 2)]\n",
      "Real tags: ['lists', 'music']\n",
      "\n",
      "The cinematography of films and TV shows\n",
      "Computed:  [('film', 25), ('lists', 9), ('animation', 3)]\n",
      "Real tags: ['colors', 'film']\n",
      "\n",
      "woscilloscope\n",
      "Computed:  []\n",
      "Real tags: ['audiodev', 'music', 'viz']\n",
      "\n",
      "Academic Earth - Free video lectures\n",
      "Computed:  [('design', 9), ('music', 9), ('gaming', 8)]\n",
      "Real tags: ['compsci', 'videos']\n",
      "\n",
      "Five Worlds\n",
      "Computed:  [('compsci', 4), ('misc', 4), ('illustration', 3)]\n",
      "Real tags: ['articles', 'misc', 'programming']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stem_fn = pst.stem\n",
    "tags_stems, stems_tags = train(stem_fn)\n",
    "\n",
    "for b in bookmarks_test[:5]:\n",
    "    print(b.title)\n",
    "    print(\"Computed: \", autotag(b.title, tags_stems, pst.stem))\n",
    "    print(\"Real tags:\", b.tags)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3831932773109244\n"
     ]
    }
   ],
   "source": [
    "# check if best tag is in real tagset\n",
    "\n",
    "success = 0\n",
    "\n",
    "for b in bookmarks_test:\n",
    "    guess = autotag(b.title, tags_stems, stem_fn)\n",
    "    if guess and guess[0][0] in b.tags:\n",
    "        success += 1\n",
    "    \n",
    "print(success / len(bookmarks_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.456 95 skipped  /  595\n"
     ]
    }
   ],
   "source": [
    "# skip if no tags were found\n",
    "\n",
    "success = 0\n",
    "skips = 0\n",
    "\n",
    "for b in bookmarks_test:\n",
    "    guess = autotag(b.title, tags_stems, stem_fn)\n",
    "    if not guess:\n",
    "        skips += 1\n",
    "        continue\n",
    "    if guess[0][0] in b.tags: \n",
    "            success += 1\n",
    "    \n",
    "print(success / (len(bookmarks_test) - skips), \n",
    "      skips, \n",
    "      \"skipped\", \" / \", \n",
    "      len(bookmarks_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6428571428571429 385 skipped  /  595\n"
     ]
    }
   ],
   "source": [
    "# only consider cases where the best tag is above a treshold\n",
    "\n",
    "success = 0\n",
    "skips = 0\n",
    "\n",
    "for b in bookmarks_test:\n",
    "    guess = autotag(b.title, tags_stems, stem_fn)\n",
    "    if not guess or guess[0][1] < 10:\n",
    "        skips += 1\n",
    "        continue\n",
    "    if guess[0][0] in b.tags: \n",
    "            success += 1\n",
    "    \n",
    "print(success / (len(bookmarks_test) - skips), \n",
    "      skips, \n",
    "      \"skipped\", \" / \", \n",
    "      len(bookmarks_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if best tag is above a treshold and check if one of the three best tags is in the real tagset\n",
    "\n",
    "success = 0\n",
    "skips = 0\n",
    "\n",
    "for b in bookmarks_test:\n",
    "    guess = autotag(b.title, tags_stems, stem_fn)\n",
    "    if not guess or guess[0][1] < 10:\n",
    "        skips += 1\n",
    "        continue\n",
    "    \n",
    "    for g in guess:\n",
    "        if g[0] in b.tags:\n",
    "            success += 1\n",
    "            break\n",
    "    \n",
    "print(success / (len(bookmarks_test) - skips), skips, \"skipped\", \" / \", len(bookmarks_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same, but using wordnet lemmatizer\n",
    "\n",
    "success = 0\n",
    "skips = 0\n",
    "\n",
    "for b in bookmarks_test:\n",
    "    guess = autotag(b.title, tags_stems, wnl.lemmatize)\n",
    "    if not guess or guess[0][1] < 10:\n",
    "        skips += 1\n",
    "        continue\n",
    "    \n",
    "    for g in guess:\n",
    "        if g[0] in b.tags:\n",
    "            success += 1\n",
    "            break\n",
    "    \n",
    "print(success / (len(bookmarks_test) - skips), skips, \"skipped\", \" / \", len(bookmarks_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autotag2(title, stems_tags, stem_fn):\n",
    "    stems = []\n",
    "    tokenized = nltk.word_tokenize(title)\n",
    "    for word, pos in nltk.pos_tag(tokenized):\n",
    "        if pos in relevant_pos:\n",
    "            stem = stem_fn(word)\n",
    "            if re.match(r\"^[\\w\\d.-]+$\", stem):\n",
    "                stems.append(stem)\n",
    "    \n",
    "    tags_points = defaultdict(int)\n",
    "    \n",
    "    for stem in stems:\n",
    "        if stem in stems_tags:\n",
    "            for tag in stems_tags[stem].items():\n",
    "                tags_points[tag[0]] += tag[1]\n",
    "        \n",
    "    return list(itertools.islice(sorted(tags_points.items(), key=lambda kv: -kv[1]), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_fn = pst.stem\n",
    "tags_stems, stems_tags = train(stem_fn)\n",
    "\n",
    "for b in bookmarks_test:\n",
    "    print(b.title)\n",
    "    print(\"Computed: \", autotag2(b.title, stems_tags, pst.stem))\n",
    "    print(\"Real tags:\", b.tags)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip if no tags were found\n",
    "\n",
    "success = 0\n",
    "skips = 0\n",
    "\n",
    "for b in bookmarks_test:\n",
    "    guess = autotag2(b.title, tags_stems, stem_fn)\n",
    "    if not guess:\n",
    "        skips += 1\n",
    "        continue\n",
    "    if guess[0][0] in b.tags: \n",
    "            success += 1\n",
    "    \n",
    "print(success / (len(bookmarks_test) - skips), skips, \"skipped\", \" / \", len(bookmarks_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autotag2(title, stems_tags, stem_fn):\n",
    "    stems = []\n",
    "    tokenized = nltk.word_tokenize(title)\n",
    "    for word, pos in nltk.pos_tag(tokenized):\n",
    "        if pos in relevant_pos:\n",
    "            stem = stem_fn(word)\n",
    "            if re.match(r\"^[\\w\\d.-]+$\", stem):\n",
    "                stems.append(stem)\n",
    "    \n",
    "    tags_points = defaultdict(int)\n",
    "    \n",
    "    for stem in stems:\n",
    "        if stem in stems_tags:\n",
    "            for tag in stems_tags[stem].items():\n",
    "                tags_points[tag[0]] += tag[1]\n",
    "        \n",
    "    return list(itertools.islice(sorted(tags_points.items(), key=lambda kv: -kv[1]), 3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
